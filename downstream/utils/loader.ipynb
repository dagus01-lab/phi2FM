{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This script demonstrates how to efficiently load and iterate over a satellite Earth observation dataset stored in the Zarr format, using a modular PyTorch DataLoader setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "GroupNotFoundError",
     "evalue": "group not found at path ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGroupNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 1. Point at the on‑disk zarr directory\u001b[39;00m\n\u001b[1;32m      8\u001b[0m store \u001b[38;5;241m=\u001b[39m DirectoryStore(zarr_path)\n\u001b[0;32m----> 9\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Print the group tree to explore its structure\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(root\u001b[38;5;241m.\u001b[39mtree())\n",
      "File \u001b[0;32m~/miniconda3/envs/phileo_env/lib/python3.9/site-packages/zarr/hierarchy.py:1578\u001b[0m, in \u001b[0;36mopen_group\u001b[0;34m(store, mode, cache_attrs, synchronizer, path, chunk_store, storage_options, zarr_version, meta_array)\u001b[0m\n\u001b[1;32m   1576\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m contains_array(store, path\u001b[38;5;241m=\u001b[39mpath):\n\u001b[1;32m   1577\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ContainsArrayError(path)\n\u001b[0;32m-> 1578\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GroupNotFoundError(path)\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1581\u001b[0m     init_group(store, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, path\u001b[38;5;241m=\u001b[39mpath, chunk_store\u001b[38;5;241m=\u001b[39mchunk_store)\n",
      "\u001b[0;31mGroupNotFoundError\u001b[0m: group not found at path ''"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "from zarr.storage import DirectoryStore\n",
    "import fsspec\n",
    "\n",
    "zarr_path = \"/Data/phisatnet_clouds/phisatnet_clouds.zarr/zarr\"\n",
    "\n",
    "# 1. Point at the on‑disk zarr directory\n",
    "store = DirectoryStore(zarr_path)\n",
    "root = zarr.open_group(store=store, mode=\"r\")\n",
    "\n",
    "# Print the group tree to explore its structure\n",
    "print(root.tree())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PathNotFoundError",
     "evalue": "nothing found at path ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPathNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mzarr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DirectoryStore\n\u001b[1;32m      4\u001b[0m store \u001b[38;5;241m=\u001b[39m DirectoryStore(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Data/phisatnet_clouds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m dataset_group \u001b[38;5;241m=\u001b[39m root[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainval\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m sample_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(dataset_group\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Data/gdaga/anaconda3/envs/esa-phisatnet/lib/python3.9/site-packages/zarr/convenience.py:137\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(store, mode, zarr_version, path, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m open_group(_store, mode\u001b[38;5;241m=\u001b[39mmode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PathNotFoundError(path)\n",
      "\u001b[0;31mPathNotFoundError\u001b[0m: nothing found at path ''"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "from zarr.storage import DirectoryStore\n",
    "\n",
    "store = DirectoryStore(\"/Data/phisatnet_clouds\")\n",
    "root = zarr.open(store=store, mode=\"r\", zarr_format=3)\n",
    "dataset_group = root[\"trainval\"]\n",
    "\n",
    "sample_id = sorted(dataset_group.keys())[0]\n",
    "sample = dataset_group[sample_id]\n",
    "img = sample[\"img\"][:]\n",
    "label = sample[\"label\"][:]\n",
    "print(f\"image shape: {img.shape}, label shape: {label.shape}\")\n",
    "# Print all metadata attributes\n",
    "print(f\"Attributes for sample '{sample_id}':\")\n",
    "for key, value in sample_group.attrs.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (8, 512, 512), label shape: (1, 512, 512)\n",
      "Attributes for sample '0000000':\n",
      "  cloud_cover: nan\n",
      "  crs: EPSG:32629\n",
      "  datatake: 21-10-2017 11:54:57\n",
      "  geolocation: {'LL': [nan, nan], 'LR': [nan, nan], 'UL': [nan, nan], 'UR': [nan, nan]}\n",
      "  sensor: S2A\n",
      "  sensor_orbit: ASCENDING\n",
      "  sensor_orbit_number: 0\n",
      "  sensor_resolution: 4.75\n",
      "  spectral_bands_ordered: B02-B03-B04-B08-B05-B06-B07-PAN\n",
      "  sun_azimuth: nan\n",
      "  sun_elevation: nan\n",
      "  task: segmentation\n",
      "  view_azimuth: nan\n",
      "  view_elevation: nan\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "from zarr.storage import DirectoryStore\n",
    "\n",
    "zarr_path = \"/Data/worldfloods/worldfloods.zarr\"\n",
    "\n",
    "root = zarr.open(zarr_path, mode='r')\n",
    "\n",
    "# Choose the split (\"trainval\" or \"test\")\n",
    "dataset_set = \"trainval\"\n",
    "dataset_group = root[dataset_set]\n",
    "\n",
    "# Access the sample group\n",
    "sample_ids = sorted(dataset_group.keys())\n",
    "\n",
    "sample_id = sample_ids[0]\n",
    "sample_group = dataset_group[sample_id]\n",
    "\n",
    "# Load image\n",
    "img = sample_group['img'][:]\n",
    "label = sample_group['label'][:]\n",
    "print(f\"image shape: {img.shape}, label shape: {label.shape}\")\n",
    "# Print all metadata attributes\n",
    "print(f\"Attributes for sample '{sample_id}':\")\n",
    "for key, value in sample_group.attrs.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes for sample '0000000':\n",
      "  cloud_cover: nan\n",
      "  datatake: 00-00-0000 00:00:00\n",
      "  geolocation: {'LL': [nan, nan], 'LR': [nan, nan], 'UL': [nan, nan], 'UR': [nan, nan]}\n",
      "  sensor: S2A\n",
      "  sensor_orbit: ASCENDING\n",
      "  sensor_orbit_number: 0\n",
      "  sensor_resolution: 10\n",
      "  spectral_bands_ordered: B2-B3-B4-B4\n",
      "  sun_azimuth: nan\n",
      "  sun_elevation: nan\n",
      "  task: segmentation\n",
      "  view_azimuth: nan\n",
      "  view_elevation: nan\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "zarr_path = \"/Data/lpl_burned_area/burned.zarr\" #\"/Data/fire_dataset/fire_dataset.zarr\"\n",
    "root = zarr.open(zarr_path, mode='r')\n",
    "\n",
    "# Choose the split (\"trainval\" or \"test\")\n",
    "dataset_set = \"trainval\"\n",
    "dataset_group = root[dataset_set]\n",
    "\n",
    "# Pick a sample ID (e.g., first one)\n",
    "sample_ids = sorted(dataset_group.keys())\n",
    "sample_id = sample_ids[0]  # or any other valid index\n",
    "\n",
    "# Access the sample group\n",
    "sample_group = dataset_group[sample_id]\n",
    "\n",
    "# Print all metadata attributes\n",
    "print(f\"Attributes for sample '{sample_id}':\")\n",
    "for key, value in sample_group.attrs.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "def collate_fn(batch: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Custom collate function to handle different task types and metadata.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of sample dictionaries from PhiSatDataset\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with batched tensors and metadata\n",
    "    \"\"\"\n",
    "    # Group samples by task to handle different label shapes\n",
    "    task_groups = {}\n",
    "    for sample in batch:\n",
    "        task = sample['task']\n",
    "        if task not in task_groups:\n",
    "            task_groups[task] = []\n",
    "        task_groups[task].append(sample)\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Process each task group separately\n",
    "    for task, samples in task_groups.items():\n",
    "        # Get all keys from the first sample\n",
    "        keys = samples[0].keys()\n",
    "        \n",
    "        for key in keys:\n",
    "            # Skip task and sample_id for batching\n",
    "            if key in ['task', 'sample_id']:\n",
    "                continue\n",
    "                \n",
    "            # Handle tensors\n",
    "            if isinstance(samples[0][key], torch.Tensor):\n",
    "                # Stack tensors with same shapes\n",
    "                try:\n",
    "                    result[f\"{task}_{key}\"] = torch.stack([s[key] for s in samples])\n",
    "                except RuntimeError:\n",
    "                    # If tensors have different shapes, return as list\n",
    "                    result[f\"{task}_{key}\"] = [s[key] for s in samples]\n",
    "            else:\n",
    "                # For non-tensor data, collect as list\n",
    "                result[f\"{task}_{key}\"] = [s[key] for s in samples]\n",
    "        \n",
    "        # Store sample IDs\n",
    "        result[f\"{task}_sample_ids\"] = [s['sample_id'] for s in samples]\n",
    "        \n",
    "    # Store task information\n",
    "    result['tasks'] = list(task_groups.keys())\n",
    "    result['task_counts'] = {task: len(samples) for task, samples in task_groups.items()}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean and std across dataset...\n",
      "Dataset trainval shapes: img=(256, 256, 7), label=(256, 256, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing stats: 100%|████████████████████████████████████████████████████████████████████████| 487/487 [01:06<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean per band: [0.5692603492540789, 0.5233146455770651, 0.49774728208504626, 0.5614061973077787, 0.5094977101466148, 0.5503450336828751, 0.5719299002762076]\n",
      "Stddev per band: [0.24279108867296925, 0.25451220952717407, 0.277410560398893, 0.28924007207410934, 0.2766535835665443, 0.2841112679453489, 0.28949325669342035]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from data_loader import get_zarr_dataloader, NormalizeChannels\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "# Path to the input Zarr dataset\n",
    "zarr_path = \"/Data/lpl_burned_area/burned.zarr\"\n",
    "# Select dataset split: \"trainval\" or \"test\"\n",
    "dataset_set = \"trainval\"\n",
    "\n",
    "# Step 1: Compute dataset-wide per-band mean and std\n",
    "print(\"Computing mean and std across dataset...\")\n",
    "_, _, dataloader = get_zarr_dataloader(\n",
    "    zarr_path=zarr_path,\n",
    "    dataset_set=dataset_set,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    task_filter=\"segmentation\",\n",
    "    metadata_keys=[\"sensor\", \"timestamp\", \"geolocation\", \"crs\"],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "sum_ = 0\n",
    "sum_sq = 0\n",
    "total_pixels = 0\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Computing stats\"):\n",
    "    for task in batch['tasks']:\n",
    "        images = np.array(batch[f'{task}_img'])  # shape: (B, H, W, C)\n",
    "        if images.ndim != 4:\n",
    "            raise ValueError(\"Expected image tensor of shape (B, H, W, C)\")\n",
    "        batch_size, height, width, _ = images.shape\n",
    "        pixels_in_batch = batch_size * height * width\n",
    "\n",
    "        sum_ += images.sum(axis=(0, 1, 2))\n",
    "        sum_sq += (images ** 2).sum(axis=(0, 1, 2))\n",
    "        total_pixels += pixels_in_batch\n",
    "\n",
    "mean = sum_ / total_pixels\n",
    "std = np.sqrt((sum_sq / total_pixels) - (mean ** 2))\n",
    "\n",
    "print(\"Mean per band:\", mean.tolist())\n",
    "print(\"Stddev per band:\", std.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean and std across dataset...\n",
      "Mean per band: [0.16345226170528732, 0.1485720135879921, 0.14293998321824464, 0.16128031952130834, 0.24135972282919374, 0.1575786149640857, 0.2155737770703539, 0.2502169078127071]\n",
      "Stddev per band: [0.28859553480558336, 0.3297414341820656, 0.3394307197049047, 0.17330132528004097, 0.2588063781866027, 0.40266684228798705, 0.40724442981311126, 0.3433434216321024]\n"
     ]
    }
   ],
   "source": [
    "from data_loader2 import get_zarr_dataloader, NormalizeChannels\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "# Path to the input Zarr dataset\n",
    "zarr_path = \"/Data/worldfloods/worldfloods.zarr\"\n",
    "# Select dataset split: \"trainval\" or \"test\"\n",
    "dataset_set = \"trainval\"\n",
    "\n",
    "# Step 1: Compute dataset-wide per-band mean and std\n",
    "print(\"Computing mean and std across dataset...\")\n",
    "#_, _, dataloader = get_zarr_dataloader(\n",
    "#    zarr_path=zarr_path,\n",
    "#    dataset_set=dataset_set,\n",
    "#    batch_size=16,\n",
    "#    shuffle=False,\n",
    "#    num_workers=4,\n",
    "#    task_filter=\"segmentation\",\n",
    "#    metadata_keys=[\"sensor\", \"timestamp\", \"geolocation\", \"crs\"],\n",
    "#    num_classes=4\n",
    "#)\n",
    "\n",
    "#sum_ = 0\n",
    "#sum_sq = 0\n",
    "#total_pixels = 0\n",
    "\n",
    "#for batch in tqdm(dataloader, desc=\"Computing stats\"):\n",
    "#    for task in batch['tasks']:\n",
    "#        try:\n",
    "#            images = np.array(batch[f'{task}_img'])  # shape: (B, H, W, C)\n",
    "#            if images.ndim != 4:\n",
    "#                raise ValueError(\"Expected image tensor of shape (B, H, W, C)\")\n",
    "#            batch_size, height, width, _ = images.shape\n",
    "#            pixels_in_batch = batch_size * height * width\n",
    "#    \n",
    "#            sum_ += images.sum(axis=(0, 1, 2))\n",
    "#            sum_sq += (images ** 2).sum(axis=(0, 1, 2))\n",
    "#            total_pixels += pixels_in_batch\n",
    "#        except Exception as e:\n",
    "#            print(e)\n",
    "#            continue\n",
    "\n",
    "mean = sum_ / total_pixels\n",
    "std = np.sqrt((sum_sq / total_pixels) - (mean ** 2))\n",
    "\n",
    "print(\"Mean per band:\", mean.tolist())\n",
    "print(\"Stddev per band:\", std.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset trainval shapes: img=(512, 512, 8), label=(512, 512, 1)\n",
      "weights: tensor([2.2159, 0.6457]), pos_weights:tensor([4.0633, 0.4754])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   6%|█                 | 228/3773 [04:35<1:31:33,  1.55s/it]"
     ]
    }
   ],
   "source": [
    "from data_loader2 import get_zarr_dataloader, NormalizeChannels\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import zarr\n",
    "\n",
    "# Path to the input Zarr dataset\n",
    "#zarr_path = \"/Data/fire_dataset/fire_dataset.zarr\"\n",
    "zarr_path = \"/Data/worldfloods/worldfloods.zarr\"\n",
    "# Select dataset split: \"trainval\" or \"test\"\n",
    "dataset_set = \"trainval\"\n",
    "#zarr.open(zarr_path)\n",
    "# Initialize a PyTorch DataLoader from a Zarr-based dataset\n",
    "_, _, dataloader = get_zarr_dataloader(\n",
    "    zarr_path=zarr_path,                     # Path to the Zarr archive\n",
    "    dataset_set=dataset_set,                 # Dataset subset to use\n",
    "    batch_size=16,                           # Number of samples per batch\n",
    "    shuffle=True,                            # Enable shuffling (useful for training)\n",
    "    num_workers=4,                           # Number of parallel workers for loading\n",
    "    #transform=NormalizeChannels(min_max=True),  # Normalize input channels to [0, 1]\n",
    "    task_filter=\"segmentation\",              # Only load data for the \"segmentation\" task\n",
    "    metadata_keys=[\"sensor\", \"timestamp\", \"geolocation\", \"crs\"],   # Include auxiliary metadata fields\n",
    ")\n",
    "\n",
    "\n",
    "all_unique_labels = set()\n",
    "\n",
    "try:\n",
    "    for idx, batch in enumerate(tqdm(dataloader, desc=\"Processing Batches\")):\n",
    "        for task in batch['tasks']:\n",
    "            labels = batch[f'{task}_label']  # Might be shape (B, H, W) or list of scalars\n",
    "    \n",
    "            # Case 1: If labels is a tensor (e.g. B x H x W)\n",
    "            if isinstance(labels, torch.Tensor):\n",
    "                unique_vals = torch.unique(labels)\n",
    "                all_unique_labels.update(unique_vals.cpu().numpy().tolist())\n",
    "    \n",
    "            # Case 2: If labels is a list/array of scalars\n",
    "            elif isinstance(labels, (list, tuple)):\n",
    "                for label in labels:\n",
    "                    if isinstance(label, torch.Tensor):\n",
    "                        unique_vals = torch.unique(label)\n",
    "                    else:\n",
    "                        # If label is scalar (e.g. float32), wrap in tensor first\n",
    "                        label_tensor = torch.tensor(label)\n",
    "                        unique_vals = torch.unique(label_tensor)\n",
    "    \n",
    "                    all_unique_labels.update(unique_vals.cpu().numpy().tolist())\n",
    "    \n",
    "            else:\n",
    "                raise TypeError(f\"Unexpected label type: {type(labels)}\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Final result\n",
    "print(f\"\\nAll unique label values seen across all batches and tasks: {sorted(all_unique_labels)}\")\n",
    "print(f\"Total number of classes: {len(all_unique_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esa-phisatnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
