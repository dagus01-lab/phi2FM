{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9104ac2c-3dfa-49fe-9dc8-e9c9b733871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d62a92-c115-4364-90f4-b638c347e493",
   "metadata": {},
   "source": [
    "# üó∫Ô∏è Major-TOM Filtering\n",
    "[![HF](https://img.shields.io/badge/%F0%9F%A4%97-Datasets-yellow)](https://www.huggingface.co/Major-TOM) [![paper](https://img.shields.io/badge/arXiv-2402.12095-D12424)](https://www.arxiv.org/abs/2402.12095) [![GitHub stars](https://img.shields.io/github/stars/ESA-PhiLab/Major-TOM?style=social&label=Star&maxAge=2592000)](https://github.com/ESA-PhiLab/Major-TOM/)\n",
    "\n",
    "This notebook demonstrates how to access MajorTOM-Core-S2L2A data quickly and filter a subset of interest.\n",
    "\n",
    "Examples:\n",
    "1. Filtering based on location, time, and cloud cover\n",
    "2. Downloading a filtered subset of the dataset\n",
    "3. PyTorch Dataset with a local copy\n",
    "4. HuggingFace `datasets` fast access via streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e315fa-1f4d-4f3c-8766-bc6d50732525",
   "metadata": {},
   "source": [
    "### 1. üìÖ Filtering based on location, time, and cloud cover\n",
    "First we will download a local copy of the dataset metadata, in this case from `Major-TOM/Core-S2L2a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518055e-c955-43c6-a5e1-0c05bc657b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "SOURCE_DATASET = 'Major-TOM/Core-S2L1C' # Identify HF Dataset\n",
    "DATASET_DIR = Path('./data/Major-TOM/')\n",
    "DATASET_DIR.mkdir(exist_ok=True, parents=True)\n",
    "ACCESS_URL = 'https://huggingface.co/datasets/{}/resolve/main/metadata.parquet?download=true'.format(SOURCE_DATASET)\n",
    "LOCAL_URL = DATASET_DIR / '{}.parquet'.format(ACCESS_URL.split('.parquet')[0].split('/')[-1])\n",
    "\n",
    "# download from server to local url\n",
    "gdf = metadata_from_url(ACCESS_URL, LOCAL_URL)\n",
    "\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebf09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gdf), type(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22f0955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "def create_map(gdf, r=3):\n",
    "    # Creating a Folium map centered around the coordinates\n",
    "    m = folium.Map(location=[0, 0], zoom_start=3, control_scale=True)\n",
    "\n",
    "    # Adding points to the map\n",
    "    for idx, row in gdf.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row['centre_lat'], row['centre_lon']],\n",
    "            radius=r,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_color='blue',\n",
    "            fill_opacity=0.7,\n",
    "            popup=f\"Cloud Cover: {row['cloud_cover']}%\",\n",
    "            tooltip=row['product_id']\n",
    "        ).add_to(m)\n",
    "\n",
    "    return m\n",
    "\n",
    "def create_heatmap(gdf, r=15, save_html=False, filename='heatmap.html'):\n",
    "    # Creating a Folium map centered around the coordinates\n",
    "    m = folium.Map(location=[0, 0], zoom_start=3, control_scale=True)\n",
    "    \n",
    "    # Adding heatmap layer\n",
    "    HeatMap(data=gdf[['centre_lat', 'centre_lon']], radius=r).add_to(m)\n",
    "\n",
    "    if save_html:\n",
    "        m.save(filename)\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "# create_map(gdf.sample(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd69033-12dc-4236-8724-65d7ebac4ff9",
   "metadata": {},
   "source": [
    "Then, we can specify a few regions using shapely geometry, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a0fcff-7df1-466d-9109-b4a64f7daf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import box\n",
    "\n",
    "# Example bounding boxes used for filtering\n",
    "switzerland = box(5.9559111595,45.8179931641,10.4920501709,47.808380127)\n",
    "gabon = box(8.1283659854,-4.9213919841,15.1618722208,2.7923006325)\n",
    "napoli = box(14.091710578,40.7915558593,14.3723765416,40.9819258062)\n",
    "pacific = box(-153.3922893485,39.6170415622,-152.0423077748,40.7090892316) # a remote patch over pacific - no data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a250931-602d-4b9b-a974-19b713712e36",
   "metadata": {},
   "source": [
    "and then use it via our `filter_metadata` function - let's try to get some recent images around ‚öΩüçï Napoli!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50b0db-249a-42e7-ac2f-b18ca682a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filter_metadata(gdf,\n",
    "                              cloud_cover = (0,10), # cloud cover between 0% and 10%\n",
    "                            #   region=switzerland, # you can try with different bounding boxes, like in the cell above\n",
    "                              daterange=('2020-01-01', '2025-01-01'), # temporal range\n",
    "                              nodata=(0.0,0.0) # only 0% of no data allowed\n",
    "                              )\n",
    "filtered_df = filtered_df[::12]\n",
    "display(filtered_df.head())\n",
    "print(f'Number of images: {len(filtered_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ca466-c540-4bfa-b878-09e0dc14b752",
   "metadata": {},
   "source": [
    "Any row from the metadata can be very easily read into a `dict` of numpy arrays using our `read_row` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c72f3406-c23d-4f54-b589-45b6511cb9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = read_row(filtered_df.iloc[0], columns = ['B04', 'thumbnail'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6b002-9f55-4c76-8bbd-b1bbcdb81667",
   "metadata": {},
   "source": [
    "### üì© Downloading a filtered subset of the dataset\n",
    "\n",
    "Use the `filter_download` function to download all files to the local directory at `local_dir`. Your new dataset will be named using `source_name`.\n",
    "\n",
    "More importantly, the `by_row` option allows to download specific rows from the archives. Set it to `True`, if you think you will take only a few files from each parquet file (most parquet files contain samples that are close to each other in space).\n",
    "\n",
    "If you expect to take most of the samples from the parquet file, setting `by_row` to `False` will probably be quicker (you then download the data as the entire file, before you rearrange it onto folders with only the files from your dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94362e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*GeoDataFrame.swapaxes.*\")\n",
    "\n",
    "n_splits = 10\n",
    "df_list = np.array_split(filtered_df, n_splits)\n",
    "\n",
    "np.array([len(df_list[i]) for i in range(n_splits)]).sum() == len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "335ab665-8238-4f17-ab4b-0bcb1fa38224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_download(df_list[0], local_dir='/home/ccollado/phileo_phisat2/MajorTOM/', source_name='L1C', by_row=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1446cd8a-ec7d-44ac-a93b-37ec0d784e8a",
   "metadata": {},
   "source": [
    "You can now check your local directory for the local version of your dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d268c-1bbb-4249-8b93-b96edb0b5fdb",
   "metadata": {},
   "source": [
    "### üî• PyTorch Dataset with a local copy\n",
    "We can use it directly with our `PyTorch` definition of the Dataset `MajorTOM`, just supply the metadata file and teh directory of the files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b241f10",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------\n",
    "# EXPERIMENT WITH DOWNLOADED DATA\n",
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21cfd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1364f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_bands = ['B02', 'B03', 'B04', 'B08', 'B05', 'B06', 'B07', 'B8A', 'B11', 'B12', 'cloud_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_csv('filtered_df.csv')\n",
    "dfs = np.array_split(filtered_df, 100)\n",
    "index_download = 1\n",
    "df_to_download = dfs[index_download]\n",
    "print(f'Subset: {len(df_to_download)} out of {len(filtered_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b33084-cc9c-4b4e-acff-e6b68e64ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MajorTOM(filtered_df, '/home/ccollado/phileo_phisat2/MajorTOM/L1C', tif_bands = tif_bands, combine_bands=False)\n",
    "\n",
    "ds[0]['meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_indices, missing_indices, df_existing, df_missing = ds.check_file_existence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a70c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_existing), len(df_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6066cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_existing.to_csv('df_existing.csv', index=False)\n",
    "# df_missing.to_csv('df_missing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a0ecf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_download(df_missing, local_dir='/home/ccollado/phileo_phisat2/MajorTOM/', source_name='L1C', by_row=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eaa6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap(filtered_df,r=10, save_html=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d5003",
   "metadata": {},
   "source": [
    "# Check Completed Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import statistics\n",
    "from tabulate import tabulate\n",
    "\n",
    "def calculate_file_stats(directory, threshold_mb=None):\n",
    "    try:\n",
    "        # List all items in the directory\n",
    "        items = os.listdir(directory)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{directory}' does not exist.\")\n",
    "        return []\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied to access '{directory}'.\")\n",
    "        return []\n",
    "\n",
    "    file_sizes_mb = []\n",
    "    zero_size_count = 0  # Counter for files with size 0\n",
    "    files_below_threshold = []  # List to store files below the threshold\n",
    "\n",
    "    for item in items:\n",
    "        filepath = os.path.join(directory, item)\n",
    "        if os.path.isfile(filepath):\n",
    "            try:\n",
    "                size_bytes = os.path.getsize(filepath)\n",
    "                if size_bytes == 0:\n",
    "                    zero_size_count += 1\n",
    "                else:\n",
    "                    size_mb = size_bytes / (1024 ** 2)  # Convert bytes to MB\n",
    "                    file_sizes_mb.append(size_mb)\n",
    "                    \n",
    "                    # Check if the file size is below the threshold\n",
    "                    if threshold_mb is not None and size_mb < threshold_mb:\n",
    "                        files_below_threshold.append((item, size_mb))\n",
    "            except OSError as e:\n",
    "                print(f\"Warning: Could not access '{filepath}'. Reason: {e}\")\n",
    "\n",
    "    if not file_sizes_mb and zero_size_count == 0:\n",
    "        print(f\"No files found in directory '{directory}'.\")\n",
    "        return []\n",
    "\n",
    "    total_size_gb = sum(file_sizes_mb) / 1024  # Convert MB to GB\n",
    "    mean_size = statistics.mean(file_sizes_mb) if file_sizes_mb else 0\n",
    "    std_dev = statistics.stdev(file_sizes_mb) if len(file_sizes_mb) > 1 else 0.0\n",
    "    min_size = min(file_sizes_mb) if file_sizes_mb else 0\n",
    "    max_size = max(file_sizes_mb) if file_sizes_mb else 0\n",
    "\n",
    "    # Prepare data for tabulation\n",
    "    table = [\n",
    "        [\"Number of files\", len(file_sizes_mb) + zero_size_count],\n",
    "        [\"Number of zero-size files\", zero_size_count],\n",
    "        [\"Total size (GB)\", f\"{total_size_gb:.2f}\"],\n",
    "        [\"Mean file size (MB)\", f\"{mean_size:.2f}\"],\n",
    "        [\"Standard deviation (MB)\", f\"{std_dev:.2f}\"],\n",
    "        [\"Minimum file size (MB)\", f\"{min_size:.2f}\"],\n",
    "        [\"Maximum file size (MB)\", f\"{max_size:.2f}\"],\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nStatistics for directory: {directory}\\n\")\n",
    "    print(tabulate(table, headers=[\"Statistic\", \"Value\"], tablefmt=\"grid\"))\n",
    "\n",
    "    # If a threshold is specified, display the list of files below the threshold\n",
    "    if threshold_mb is not None:\n",
    "        if files_below_threshold:\n",
    "            print(f\"\\nFiles below {threshold_mb} MB:\")\n",
    "            # Prepare table for files below threshold\n",
    "            threshold_table = [\n",
    "                [filename, f\"{size_mb:.2f} MB\"] for filename, size_mb in files_below_threshold\n",
    "            ]\n",
    "            print(tabulate(threshold_table, headers=[\"Filename\", \"Size\"], tablefmt=\"grid\"))\n",
    "        else:\n",
    "            print(f\"\\nNo files are below {threshold_mb} MB.\")\n",
    "\n",
    "    return [filename for filename, _ in files_below_threshold] if threshold_mb is not None else []\n",
    "\n",
    "\n",
    "directory_path = '/home/ccollado/phileo_phisat2/MajorTOM/tiff_files'\n",
    "size_threshold = 20.0  # Threshold in MB (e.g., 1 MB)\n",
    "\n",
    "files_below = calculate_file_stats(directory_path, threshold_mb=size_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a82774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing = pd.read_csv('df_existing.csv')\n",
    "df_simulation = df_existing.copy().iloc[::6]\n",
    "\n",
    "df_simulation['unique_identifier'] = (\n",
    "    df_simulation['product_id'].astype(str) + '__' +\n",
    "    df_simulation['grid_row_u'].astype(str) + '_' +\n",
    "    df_simulation['grid_col_r'].astype(str)\n",
    ")\n",
    "\n",
    "df_simulation.to_csv('df_simulation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b532469",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap(gdf, r=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import string\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(name):\n",
    "    valid_chars = f\"-_.() {string.ascii_letters}{string.digits}\"\n",
    "    sanitized = ''.join(c for c in name if c in valid_chars)\n",
    "    return sanitized\n",
    "\n",
    "# Ensure required columns exist\n",
    "if not {'product_id', 'unique_identifier'}.issubset(df_simulation.columns):\n",
    "    raise ValueError(\"DataFrame must contain 'product_id' and 'unique_identifier' columns.\")\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = Path('/home/ccollado/phileo_phisat2/MajorTOM/tiff_files')  # Replace with your actual folder path\n",
    "\n",
    "if not folder_path.exists():\n",
    "    raise FileNotFoundError(f\"The folder path {folder_path} does not exist.\")\n",
    "\n",
    "# Create the mapping dictionary\n",
    "id_mapping = pd.Series(df_simulation.unique_identifier.values,\n",
    "                       index=df_simulation.product_id).to_dict()\n",
    "\n",
    "# Iterate and rename files\n",
    "for file in folder_path.glob('*.tif'):\n",
    "    product_id = file.stem\n",
    "    unique_id = id_mapping.get(product_id)\n",
    "\n",
    "    if unique_id:\n",
    "        sanitized_unique_id = sanitize_filename(unique_id)\n",
    "        new_filename = f\"{sanitized_unique_id}.tif\"\n",
    "        new_file_path = folder_path / new_filename\n",
    "\n",
    "        if new_file_path.exists():\n",
    "            print(f\"Warning: {new_filename} already exists. Skipping renaming of {file.name}.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            file.rename(new_file_path)\n",
    "            print(f\"Renamed '{file.name}' to '{new_filename}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error renaming {file.name} to {new_filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"No unique_identifier found for product_id '{product_id}'. Skipping file '{file.name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92690607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Calculate the counts of each product_id\n",
    "product_counts = df_existing['product_id'].value_counts()\n",
    "\n",
    "# Step 2: Filter product_ids with counts greater than 1\n",
    "product_ids_gt1 = product_counts[product_counts > 1].index\n",
    "\n",
    "# Option 1: Using List Comprehension\n",
    "files = os.listdir('/home/ccollado/phileo_phisat2/MajorTOM/tiff_files')\n",
    "files = [f[:-4] for f in files if f.endswith('.tif')]\n",
    "\n",
    "matching_files = [file for file in files if file in product_ids_gt1]\n",
    "\n",
    "# Option 2: Using Set Intersection for Better Performance (Especially with Large Lists)\n",
    "files_set = set(files)\n",
    "product_ids_set = set(product_ids_gt1)\n",
    "matching_files = list(files_set & product_ids_set)\n",
    "len(matching_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the files\n",
    "total_deleted = 0\n",
    "for file_path in matching_files:\n",
    "    file_path_tif = os.path.join(directory_path, file_path + '.tif')\n",
    "    try:\n",
    "        os.remove(file_path_tif)\n",
    "        print(f\"Deleted: {file_path_tif}\")\n",
    "        total_deleted += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path_tif}\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: {file_path_tif}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {file_path_tif}: {e}\")\n",
    "\n",
    "print(f\"Total files deleted: {total_deleted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_file_sizes(directory):\n",
    "    file_sizes_mb = []\n",
    "    zero_size_count = 0  # Counter for files with size 0\n",
    "\n",
    "    try:\n",
    "        items = os.listdir(directory)\n",
    "        for item in items:\n",
    "            filepath = os.path.join(directory, item)\n",
    "            if os.path.isfile(filepath):\n",
    "                size_bytes = os.path.getsize(filepath)\n",
    "                if size_bytes == 0:\n",
    "                    zero_size_count += 1\n",
    "                else:\n",
    "                    size_mb = size_bytes / (1024 ** 2)  # Convert bytes to MB\n",
    "                    file_sizes_mb.append(size_mb)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while accessing directory: {e}\")\n",
    "\n",
    "    return file_sizes_mb, zero_size_count\n",
    "\n",
    "def plot_histogram(file_sizes, plt_title='Distribution of File Sizes'):\n",
    "    if not file_sizes:\n",
    "        print(\"No non-zero files to plot.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(file_sizes, bins=30, edgecolor='black')\n",
    "    plt.title(plt_title)\n",
    "    plt.xlabel('File Size (MB)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "directory_path = '/home/ccollado/phileo_phisat2/MajorTOM/tiff_files'\n",
    "file_sizes, zero_size_count = get_file_sizes(directory_path)\n",
    "plot_histogram(file_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145293d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_recent_file_sizes(directory, hours=2):\n",
    "    current_time = time.time()\n",
    "    time_threshold = current_time - (hours * 3600)  # Convert hours to seconds\n",
    "    file_sizes_mb = []\n",
    "    zero_size_count = 0  # Counter for files with size 0\n",
    "\n",
    "    try:\n",
    "        items = os.listdir(directory)\n",
    "        for item in items:\n",
    "            filepath = os.path.join(directory, item)\n",
    "            if os.path.isfile(filepath):\n",
    "                file_creation_time = os.path.getctime(filepath)\n",
    "                if file_creation_time >= time_threshold:\n",
    "                    size_bytes = os.path.getsize(filepath)\n",
    "                    if size_bytes == 0:\n",
    "                        zero_size_count += 1\n",
    "                    else:\n",
    "                        size_mb = size_bytes / (1024 ** 2)  # Convert bytes to MB\n",
    "                        file_sizes_mb.append(size_mb)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while accessing directory: {e}\")\n",
    "\n",
    "    return file_sizes_mb, zero_size_count\n",
    "\n",
    "hours = 1\n",
    "recent_file_sizes, recent_zero_size_count = get_recent_file_sizes(directory_path, hours=1)\n",
    "plot_histogram(recent_file_sizes, plt_title=f'Distribution of File Sizes in the Last {hours} Hours')\n",
    "\n",
    "if recent_zero_size_count > 0:\n",
    "    print(f\"Number of zero-size files in the last 2 hours: {recent_zero_size_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6bc08e-1c90-48be-b2db-8de0bf1ee339",
   "metadata": {},
   "source": [
    "### Ecco!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ds[0].keys():\n",
    "    if type(ds[0][key])==torch.Tensor:\n",
    "        print(key, ds[0][key].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06eccad-7a91-4b63-944b-b6e94bf28a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Keys: {ds[0].keys()}')\n",
    "if \"B04\" in ds[0].keys():\n",
    "    print(f'Shape: {ds[0][\"B04\"].shape}')\n",
    "    print(f'dtype: {ds[0][\"B04\"].dtype}')\n",
    "elif \"bands\" in ds[0].keys():\n",
    "    print(f'Shape: {ds[0][\"bands\"].shape}')\n",
    "    print(f'dtype: {ds[0][\"bands\"].dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1df280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b957424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f0c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArrayDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        x_data, \n",
    "        y_data, \n",
    "        transform_x=None, \n",
    "        transform_y=None,\n",
    "        apply_zoom_task=True,\n",
    "        apply_reconstruction_task=True,\n",
    "        zoom_range=(1.0, 2.0),\n",
    "        augment_drop=None,\n",
    "        device='cpu',\n",
    "        clip_values=(-3.0, 3.0)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_data (MultiArray): Input features (H, W, C).\n",
    "            y_data (dict): Dictionary with keys like 'coords', 'climate'.\n",
    "            transform_x (callable, optional): Optional transform on x_data.\n",
    "            transform_y (callable, optional): Optional transform on y_data.\n",
    "            apply_zoom_task (bool): Whether to apply the zoom-level prediction task.\n",
    "            apply_reconstruction_task (bool): Whether to apply the masking reconstruction task.\n",
    "            zoom_range (tuple): Range (min_zoom, max_zoom) for zoom factor.\n",
    "            augment_drop (callable): Transformations (RandomErasing) for masking rectangular areas.\n",
    "            device (str): 'cpu' or 'cuda'.\n",
    "        \"\"\"\n",
    "        self.x_data = x_data\n",
    "        self.y_coords = y_data['coords']\n",
    "        self.y_climate = y_data['climate']\n",
    "        self.transform_x = transform_x\n",
    "        self.transform_y = transform_y\n",
    "        self.apply_zoom_task = apply_zoom_task\n",
    "        self.apply_reconstruction_task = apply_reconstruction_task\n",
    "        self.zoom_range = zoom_range\n",
    "        self.augment_drop = augment_drop\n",
    "        self.device = device\n",
    "        self.clip_values = clip_values\n",
    "        self.num_classes = 31\n",
    "\n",
    "        if not (len(self.x_data) == len(self.y_coords) == len(self.y_climate)):\n",
    "            raise ValueError(\"x_data, y_coords, and y_climate must have the same length.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def zero_to_noise(self, image):\n",
    "        \"\"\"\n",
    "        Applies random erasing transformations to the combined image and a white image to identify erased areas.\n",
    "        Then replaces the erased areas in the original image with noise.\n",
    "\n",
    "        image: (C, H, W) torch tensor\n",
    "        \"\"\"\n",
    "        mean_val = torch.mean(image)\n",
    "        std_val = torch.std(image) + 1e-6\n",
    "\n",
    "        noise = torch.normal(mean=mean_val, std=std_val, size=image.size(), device=self.device)\n",
    "        noise = torch.clamp(noise, torch.min(image), torch.max(image))\n",
    "\n",
    "        # Create a white image\n",
    "        white = torch.ones_like(image, device=self.device)\n",
    "\n",
    "        # Concatenate original and white along the channel dimension\n",
    "        # merged: (2*C, H, W)\n",
    "        merged = torch.cat([image, white], dim=0)\n",
    "\n",
    "        # Apply random erasing transforms\n",
    "        dropped = self.augment_drop(merged)  # Should erase areas in both parts\n",
    "\n",
    "        # The second half of channels correspond to white image\n",
    "        C, H, W = image.shape\n",
    "        erased_mask = (dropped[C:2*C, :, :] == 0)\n",
    "\n",
    "        # Replace erased areas in the original with noise\n",
    "        reconstructed = torch.where(erased_mask, noise, dropped[:C, :, :])\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "    def augment_drop_fn(self, image):\n",
    "        # Just apply zero_to_noise to the entire multispectral image at once\n",
    "        return self.zero_to_noise(image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load raw data\n",
    "        x = self.x_data[idx]           # NumPy array (H, W, C)\n",
    "        y_coords = self.y_coords[idx]  # NumPy array\n",
    "        y_climate = self.y_climate[idx]# NumPy array\n",
    "\n",
    "        # Convert to torch and permute to (C, H, W)\n",
    "        x = torch.tensor(x, dtype=torch.float32, device=self.device).permute(2, 0, 1)\n",
    "        y_coords = torch.tensor(y_coords, dtype=torch.float32, device=self.device)\n",
    "        y_climate = torch.tensor(y_climate, dtype=torch.float32, device=self.device).permute(2, 0, 1)\n",
    "\n",
    "        if self.transform_x is not None:\n",
    "            x, y_climate = self.transform_x(x, y_climate)  # Modified transform that handles both x and y_climate\n",
    "\n",
    "        if self.transform_y:\n",
    "            y_coords = self.transform_y(y_coords)\n",
    "            y_climate = self.transform_y(y_climate)\n",
    "\n",
    "        # Label dictionary\n",
    "        y = {'coords': y_coords}\n",
    "\n",
    "        # ---------------------------\n",
    "        # Self-Supervised: Zoom Task\n",
    "        # ---------------------------\n",
    "        if self.apply_zoom_task:\n",
    "            zoom_factor = random.uniform(*self.zoom_range)\n",
    "            C, H, W = x.shape\n",
    "            new_H, new_W = int(H * zoom_factor), int(W * zoom_factor)\n",
    "            \n",
    "            # Resize x\n",
    "            zoomed = F.resize(x, (new_H, new_W), antialias=True)\n",
    "            \n",
    "            # Resize y_climate similarly\n",
    "            zoomed_climate = F.resize(y_climate, (new_H, new_W), antialias=True)\n",
    "\n",
    "            if zoom_factor >= 1.0:\n",
    "                # Center crop to original size\n",
    "                top = (new_H - H) // 2\n",
    "                left = (new_W - W) // 2\n",
    "                x_zoomed = zoomed[:, top:top+H, left:left+W]\n",
    "                y_climate_zoomed = zoomed_climate[:, top:top+H, left:left+W]\n",
    "            else:\n",
    "                # If zoom_factor < 1.0, pad instead\n",
    "                x_zoomed = torch.zeros(C, H, W, device=self.device)\n",
    "                y_climate_zoomed = torch.zeros_like(y_climate)\n",
    "\n",
    "                pad_h = (H - new_H) // 2\n",
    "                pad_w = (W - new_W) // 2\n",
    "                x_zoomed[:, pad_h:pad_h+new_H, pad_w:pad_w+new_W] = zoomed\n",
    "                y_climate_zoomed[:, pad_h:pad_h+new_H, pad_w:pad_w+new_W] = zoomed_climate\n",
    "\n",
    "            x = x_zoomed\n",
    "            y_climate_one_hot = Ftorch.one_hot(y_climate_zoomed.to(torch.int64), num_classes=self.num_classes)\n",
    "            y_climate_one_hot = y_climate_one_hot.permute(3, 1, 2, 0).squeeze(3).to(torch.float32)\n",
    "            y['climate'] = y_climate_one_hot\n",
    "            y['zoom_factor'] = torch.tensor(zoom_factor, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            \n",
    "        else:\n",
    "            y_climate_one_hot = Ftorch.one_hot(y_climate.to(torch.int64), num_classes=self.num_classes)\n",
    "            y_climate_one_hot = y_climate_one_hot.permute(3, 1, 2, 0).squeeze(3).to(torch.float32)\n",
    "            y['climate'] = y_climate_one_hot\n",
    "            y['zoom_factor'] = torch.tensor(1.0, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Self-Supervised: Reconstruction Task\n",
    "        # ---------------------------\n",
    "        if self.apply_reconstruction_task and self.augment_drop is not None:\n",
    "            x_original = x.clone()\n",
    "            # Mask out areas in the image using RandomErasing transformations\n",
    "            x_masked = self.augment_drop_fn(x)\n",
    "            y['reconstruction'] = x_original\n",
    "            x = torch.clip(x_masked, self.clip_values[0], self.clip_values[1])\n",
    "        else:\n",
    "            x = torch.clip(x, self.clip_values[0], self.clip_values[1])\n",
    "            y['reconstruction'] = None\n",
    "\n",
    "        # print(f'out shape: {x.shape}, {y[\"coords\"].shape}, {y[\"climate\"].shape}', {y[\"zoom_factor\"].shape}, y[\"reconstruction\"].shape)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "class TransformX(nn.Module):\n",
    "    def __init__(self, device, means, stds, augmentations, clip_values=(-3.0, 3.0), rot_prob=0.2, flip_prob=0.2, noise_prob=0.2, noise_std_range=(0.005, 0.01)):\n",
    "        super(TransformX, self).__init__()\n",
    "        self.device = device\n",
    "        self.means = (means.view(-1, 1, 1)).to(device)\n",
    "        self.stds = (stds.view(-1, 1, 1)).to(device)\n",
    "        self.augmentations = augmentations\n",
    "        self.clip_min, self.clip_max = clip_values\n",
    "        self.rot_prob = rot_prob\n",
    "        self.flip_prob = flip_prob\n",
    "        self.noise_prob = noise_prob\n",
    "        self.noise_std_low, self.noise_std_high = noise_std_range\n",
    "        self.rotations = 4  # 0, 90, 180, 270 degrees\n",
    "\n",
    "    def forward(self, x, y_climate):\n",
    "        # x: (C, H, W)\n",
    "        # y_climate: (H, W) or (C', H, W) depending on label format\n",
    "\n",
    "        # Spatial transforms\n",
    "        if self.augmentations:\n",
    "            # 1. Rotation\n",
    "            if torch.rand(1).item() < self.rot_prob:\n",
    "                k = random.randint(0, self.rotations - 1)\n",
    "                x = torch.rot90(x, k, [1, 2])\n",
    "                if y_climate is not None:\n",
    "                    y_climate = torch.rot90(y_climate, k, [0, 1]) if y_climate.dim() == 2 else torch.rot90(y_climate, k, [1,2])\n",
    "\n",
    "            # 2. Flips\n",
    "            # Horizontal flip (flip width axis)\n",
    "            if torch.rand(1).item() < self.flip_prob:\n",
    "                x = torch.flip(x, [2])\n",
    "                if y_climate is not None:\n",
    "                    y_climate = torch.flip(y_climate, [1] if y_climate.dim() == 2 else [2])\n",
    "\n",
    "            # Vertical flip (flip height axis)\n",
    "            if torch.rand(1).item() < self.flip_prob:\n",
    "                x = torch.flip(x, [1])\n",
    "                if y_climate is not None:\n",
    "                    y_climate = torch.flip(y_climate, [0] if y_climate.dim() == 2 else [1])\n",
    "\n",
    "        # Non-spatial transforms\n",
    "        # Normalize x (not y_climate)\n",
    "        x = x / 10000.0\n",
    "        x = (x - self.means) / self.stds\n",
    "\n",
    "        if self.augmentations:\n",
    "            # Add noise\n",
    "            if torch.rand(1).item() < self.noise_prob:\n",
    "                noise_std = random.uniform(self.noise_std_low, self.noise_std_high)\n",
    "                noise = torch.randn_like(x) * noise_std\n",
    "                x = x + noise\n",
    "\n",
    "        x = torch.clamp(x, self.clip_min, self.clip_max)\n",
    "\n",
    "        return x, y_climate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ae935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phileo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
